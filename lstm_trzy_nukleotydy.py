# -*- coding: utf-8 -*-
"""LSTM_trzy_nukleotydy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZfwqWz93A3Mzcr71gNNls8sVzOEMDgn7
"""

import torch
import re
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchtext
import numpy as np
import matplotlib.pyplot as plt
import random
import io
import os
import torchtext


with open('fasta_essentials_nt.dat','r') as f:
    e_file = f.read()

with open('nonessentials_nt.dat','r') as f1:
    n_file = f1.read()
    
x = re.split('>(DEG\d+)\n([ATGC\n]+)', e_file)
v = [i.replace('\n','') for i in x]

x2 = re.split('>(DNEG\d+)\n([ATGC\n]+)', n_file)
v2 = [i.replace('\n','') for i in x2]

e_names = []
e_seq = []

for i in v:
    match1= re.search('^[ATGC]+$',i)
    match2= re.search('>?DEG',i)
    if match2:
        if not 'available' in i:
            e_names.append(i)
    else:
        if match1:
            e_seq.append(i)

n_names = []
n_seq = []

for i in v2:
    match1= re.search('^[ATGC]+$',i)
    match2= re.search('>?DNEG',i)
    if match2:
        if not 'available' in i:
            n_names.append(i)
    else:
        if match1:
            n_seq.append(i)

def seq_with_label(seq, label):
    
    words_labels = []
    
    for s in seq:
            
        words_labels.append([label, s])
            
    return words_labels

e_data = seq_with_label(e_seq, 'e')
ne_data = seq_with_label(n_seq, 'n')

e_len = len(e_data)

data = e_data + ne_data[:e_len]

random.shuffle(data)

train_l = []
valid_l = []
test_l = []

for i, batch in enumerate(data):
  if i % 5 < 3:    
    train_l.append(batch) # 60% danych treningowych
  elif i % 5 == 3: #czyli 3
    valid_l.append(batch) # 20% danych do walidacji
  else:            #czyli gddy i%5 jest 4
    test_l.append(batch) # 20% danych testowych

import csv

with open('genes_with_labels.tsv', 'wt') as file:
    tsv_writer = csv.writer(file, delimiter='\t')
    for elem in data:
      tsv_writer.writerow(elem)

with open('train.tsv', 'wt') as file:
    tsv_writer = csv.writer(file, delimiter='\t')
    for elem in train_l:
      tsv_writer.writerow(elem)

with open('valid.tsv', 'wt') as file:
    tsv_writer = csv.writer(file, delimiter='\t')
    for elem in valid_l:
      tsv_writer.writerow(elem)

with open('test.tsv', 'wt') as file:
    tsv_writer = csv.writer(file, delimiter='\t')
    for elem in test_l:
      tsv_writer.writerow(elem)

def tokenizator(seq):
    reprezentacja = []
    for i in range(len(seq)-2):
        reprezentacja.append(seq[i:(i+3)])
    return reprezentacja

text_field = torchtext.legacy.data.Field(sequential=True,      
                                  tokenize=tokenizator, 
                                  include_lengths=True, 
                                  batch_first=True,
                                  use_vocab=True)      

label_field = torchtext.legacy.data.Field(sequential=False,    
                                   use_vocab=False,     
                                   is_target=True,      
                                   batch_first=True,
                                   preprocessing=lambda x: int(x == 'e')) 

fields = [('label', label_field), ('seq', text_field)]

train = torchtext.legacy.data.TabularDataset('train.tsv', "tsv", fields)
test = torchtext.legacy.data.TabularDataset('test.tsv', "tsv", fields)
valid = torchtext.legacy.data.TabularDataset('valid.tsv', "tsv", fields)

text_field.build_vocab(train)

torchtext_train_dataloader, torchtext_valid_dataloader = torchtext.legacy.data.BucketIterator.splits((train, valid),
                                           batch_size=4,
                                           sort_key=lambda x: len(x.seq), 
                                           sort_within_batch=True,        
                                           repeat=False)

class T_LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) #LSTM
        self.fc = nn.Linear(hidden_size, num_classes)  #przeksztalcenie liniowe
    
    def forward(self, x):
        x = x.unsqueeze(2)
        x = torch.tensor(x)
        x = torch.tensor(x, dtype=torch.float)
        h0 = torch.zeros(1, x.size(0), self.hidden_size) #początkowy  h0
        c0 = torch.zeros(1, x.size(0), self.hidden_size) #początkowy c0
        out, _ = self.lstm(x, (h0, c0))  #LSTM
        out = self.fc(out[:, -1, :]) #przeksztlcam jeszcze liniowo ostatni output
        return out

lstm_model = T_LSTM(1, 5, 2)

#Funkcja wyznaczająca dokładność predykcji:

def get_accuracy(model, data_loader):
    data_loader.create_batches()
    correct, total = 0, 0  #ile ok, ile wszystkich

    for num, batch in enumerate(data_loader): #przechodzi dane

        # Put all example.text of batch in single array.
        batch_text = list(batch)[0]
        labels = list(batch)[1]
        x = torch.tensor(batch_text[0])
        output = model(x) #co mowi model
        pred = output.max(1, keepdim=True)[1]  #ktora kategoria
        #print(pred)
        correct += pred.eq(labels.view_as(pred)).sum().item()
        #print(correct)
        total += labels.shape[0]
    return correct / total

#funkcja do trenowania modelu

def train_model(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-5):
    criterion = nn.CrossEntropyLoss() #funkcja kosztu
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #optymalizator modelu
    losses, train_acc, valid_acc, epochs = [], [], [], []  #cztery listy na wartosci funkcji kosztu, dokladnosc na zbiorze testowym i walidacyjnym, numer epoki
    
    for epoch in range(num_epochs): #przechodz kolejne epoki (iteracje)
        
        #train_loader.create_batches()
        
        for num, batch in enumerate(train_loader): #.batches):
            
            # Put all example.text of batch in single array.
            batch_text = list(batch)[0]
            batch_label = list(batch)[1]
            optimizer.zero_grad()
            x = torch.tensor(batch_text[0])
            pred = model(x)
            loss = criterion(pred, batch_label)  #wartosc funkcji kosztu - porownanie tego co mowi model, a tego jak jest
            loss.backward()                  #pochodna po funkcji kosztu
            optimizer.step()                 #aktualizacja parametrow

            
        losses.append(float(loss))           #zapisz aktualną wartosc funkcji kosztu
        epochs.append(epoch)                 #zapisz aktualny numer epoki
        train_acc.append(get_accuracy(model, torchtext_train_dataloader))   #dokladnosc na zbiorze treningowym
        valid_acc.append(get_accuracy(model, torchtext_valid_dataloader))   #dokladnosc na zbiorze walidacyjnym
        print(f'Epoch number: {epoch+1} | Loss value: {loss} | Train accuracy: {round(train_acc[-1],3)} | Valid accuracy: {round(valid_acc[-1],3)}')

train_model(lstm_model, torchtext_train_dataloader, torchtext_valid_dataloader, num_epochs=35, learning_rate=0.0001)
import pickle
filehandler = open(b"3nuc_model","wb")
pickle.dump(lstm_model,filehandler)

